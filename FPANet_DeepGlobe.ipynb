{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FPA-Net: Frequency-Guided Position-Based Attention Network for Land Cover Image Segmentation"
      ],
      "metadata": {
        "id": "N96AAUBLuNKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Author: **<small><small>  AL SHAHRIAR RUBEL </small></small>**\n",
        "\n"
      ],
      "metadata": {
        "id": "Nq0w8awfswQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Citation\n",
        "Please **cite** the following research article if you use any part of this work <br><br>\n",
        "***Rubel, Al Shahriar, and Frank Y. Shih. \"FPA-Net: Frequency-guided Position-based Attention Network for Land Cover Image Segmentation.\" International Journal of Pattern Recognition and Artificial Intelligence (2023).***"
      ],
      "metadata": {
        "id": "EVDaOdz8ttY7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn85nQ2RFQGL"
      },
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY5ari5EyJS7"
      },
      "outputs": [],
      "source": [
        "# Connecting Google Drive with Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WuaPfK4YLQj"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install -U albumentations\n",
        "!pip install -q torchinfo\n",
        "!pip install pandas\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k29CKyvhYPsa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnEECNYmE40t"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(42, workers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWeLvRPAYZHn"
      },
      "source": [
        "Extract Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pYjuBJ-YUM1"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 320\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "\n",
        "color_dict = pd.read_csv('drive/MyDrive/MLProject/data/class_dict.csv')\n",
        "CLASSES = color_dict['name']\n",
        "print(color_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkLWO5X_YbpA"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "pd_dataset = pd.DataFrame({'IMAGES': sorted(glob(\"drive/MyDrive/MLProject/data/train/*_sat.jpg\")), 'MASKS': sorted(glob(\"drive/MyDrive/MLProject/data/train/*_mask.png\"))\n",
        "})\n",
        "pd_dataset = shuffle(pd_dataset)\n",
        "pd_dataset.reset_index(inplace=True, drop=True)\n",
        "pd_dataset.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDfNjZN7Yhae"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd_train, pd_test = train_test_split(pd_dataset, test_size=0.25, random_state=0)\n",
        "pd_train, pd_val = train_test_split(pd_train, test_size=0.2, random_state=0)\n",
        "\n",
        "print(\"Training set size:\", len(pd_train))\n",
        "print(\"Validation set size:\", len(pd_val))\n",
        "print(\"Testing set size:\", len(pd_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JmMmhaeYoef"
      },
      "outputs": [],
      "source": [
        "index = 200\n",
        "\n",
        "sample_img = cv2.imread(pd_train.iloc[index].IMAGES)\n",
        "sample_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "sample_msk = cv2.imread(pd_train.iloc[index].MASKS)\n",
        "sample_msk = cv2.cvtColor(sample_msk, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
        "\n",
        "ax1.set_title('IMAGE')\n",
        "ax1.imshow(sample_img)\n",
        "\n",
        "ax2.set_title('MASK')\n",
        "ax2.imshow(sample_msk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDH5bPz3Y0_Q"
      },
      "source": [
        "utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7x5tScpYr_3"
      },
      "outputs": [],
      "source": [
        "def rgb2category(rgb_mask):\n",
        "    category_mask = np.zeros(rgb_mask.shape[:2], dtype=np.int8)\n",
        "    for i, row in color_dict.iterrows():\n",
        "        category_mask += (np.all(rgb_mask.reshape((-1, 3)) == (row['r'], row['g'], row['b']), axis=1).reshape(rgb_mask.shape[:2]) * i)\n",
        "    return category_mask\n",
        "\n",
        "def category2rgb(category_mask):\n",
        "    rgb_mask = np.zeros(category_mask.shape[:2] + (3,))\n",
        "    for i, row in color_dict.iterrows():\n",
        "        rgb_mask[category_mask==i] = (row['r'], row['g'], row['b'])\n",
        "    return np.uint8(rgb_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osCTCB_DY3qD"
      },
      "source": [
        "Data Augmentations & Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMNi2AcCYs2O"
      },
      "outputs": [],
      "source": [
        "import albumentations as aug\n",
        "\n",
        "train_augment = aug.Compose([\n",
        "    aug.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    aug.HorizontalFlip(p=0.5),\n",
        "    aug.VerticalFlip(p=0.5),\n",
        "    aug.RandomBrightnessContrast(p=0.3)\n",
        "])\n",
        "\n",
        "test_augment = aug.Compose([\n",
        "    aug.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    aug.RandomBrightnessContrast(p=0.3)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQQM-iffZAsH"
      },
      "source": [
        "Create PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vpks_KNZFQI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, df, augmentations=None):\n",
        "        self.df = df\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        row = self.df.iloc[index]\n",
        "\n",
        "        image = cv2.imread(row.IMAGES)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(row.MASKS)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.augmentations:\n",
        "            data = self.augmentations(image=image, mask=mask)\n",
        "            image = data['image']\n",
        "            mask = data['mask']\n",
        "\n",
        "        mask = rgb2category(mask)\n",
        "\n",
        "        image = np.transpose(image, (2, 0, 1)).astype(np.float64)\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "\n",
        "        image = torch.Tensor(image) / 255.0\n",
        "        mask = torch.Tensor(mask).long()\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OLzOGkmZKcm"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, pd_train, pd_val, pd_test, batch_size=10):\n",
        "        super().__init__()\n",
        "        self.pd_train = pd_train\n",
        "        self.pd_val = pd_val\n",
        "        self.pd_test = pd_test\n",
        "        self.batch_size=batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = SegmentationDataset(self.pd_train, train_augment)\n",
        "        self.val_dataset = SegmentationDataset(self.pd_val, test_augment)\n",
        "        self.test_dataset = SegmentationDataset(self.pd_test, test_augment)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size // 2, shuffle=False, num_workers=1)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size // 2, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo6rFzt9ZPWf"
      },
      "outputs": [],
      "source": [
        "data_module = SegmentationDataModule(pd_train, pd_val, pd_test, batch_size=BATCH_SIZE)\n",
        "data_module.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKHBWGwAZSQe"
      },
      "outputs": [],
      "source": [
        "image, mask = next(iter(data_module.train_dataloader()))\n",
        "image.shape, mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLDr1CDqPidu"
      },
      "source": [
        "### Model Preparation (Before Training)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-dct"
      ],
      "metadata": {
        "id": "76mJZ86cQIYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "qyVRhIhUQOdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nup9IQY08wqk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_dct as dct\n",
        "\n",
        "from segmentation_models_pytorch.base import modules as md\n",
        "\n",
        "class FrequencyFeature(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding = 'same')\n",
        "    def forward(self, im):\n",
        "        f_threshold = 10\n",
        "        filtermask = torch.ones(im.shape[2],im.shape[3]).to(DEVICE)\n",
        "        for i in range(f_threshold+1):\n",
        "          for j in range(f_threshold+1):\n",
        "            if j<=f_threshold - i:\n",
        "              filtermask[i,j] = 0\n",
        "        x = dct.dct_2d(im)\n",
        "        x = x * filtermask\n",
        "\n",
        "        x = dct.idct_2d(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PAB(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, pab_channels=64):\n",
        "        super(PAB, self).__init__()\n",
        "        self.pab_channels = pab_channels\n",
        "        self.in_channels = in_channels\n",
        "        self.top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
        "        self.center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
        "        self.bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.map_softmax = nn.Softmax(dim=1)\n",
        "        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bsize = x.size()[0]\n",
        "        h = x.size()[2]\n",
        "        w = x.size()[3]\n",
        "        x_top = self.top_conv(x)\n",
        "        x_center = self.center_conv(x)\n",
        "        x_bottom = self.bottom_conv(x)\n",
        "\n",
        "        x_top = x_top.flatten(2)\n",
        "        x_center = x_center.flatten(2).transpose(1, 2)\n",
        "        x_bottom = x_bottom.flatten(2).transpose(1, 2)\n",
        "\n",
        "        sp_map = torch.matmul(x_center, x_top)\n",
        "        sp_map = self.map_softmax(sp_map.view(bsize, -1)).view(bsize, h * w, h * w)\n",
        "        sp_map = torch.matmul(sp_map, x_bottom)\n",
        "        sp_map = sp_map.reshape(bsize, self.in_channels, h, w)\n",
        "        x = x + sp_map\n",
        "        x = self.out_conv(x)\n",
        "        return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True):\n",
        "        super().__init__()\n",
        "        self.conv1 = md.Conv2dReLU(\n",
        "            in_channels + skip_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.conv2 = md.Conv2dReLU(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FPAnetDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_channels,\n",
        "        decoder_channels,\n",
        "        n_blocks=5,\n",
        "        reduction=16,\n",
        "        use_batchnorm=True,\n",
        "        pab_channels=64,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if n_blocks != len(decoder_channels):\n",
        "            raise ValueError(\n",
        "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
        "                    n_blocks, len(decoder_channels)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # remove first skip with same spatial resolution\n",
        "        encoder_channels = encoder_channels[1:]\n",
        "\n",
        "        # reverse channels to start from head of encoder\n",
        "        encoder_channels = encoder_channels[::-1]\n",
        "\n",
        "        # computing blocks input and output channels\n",
        "        head_channels = encoder_channels[0]\n",
        "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
        "        skip_channels = list(encoder_channels[1:]) + [0]\n",
        "        out_channels = decoder_channels\n",
        "\n",
        "        self.center = PAB(head_channels, head_channels, pab_channels=pab_channels)\n",
        "\n",
        "        # combine decoder keyword arguments\n",
        "        kwargs = dict(use_batchnorm=use_batchnorm)  # no attention type here\n",
        "\n",
        "        blocks = [\n",
        "            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
        "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
        "        ]\n",
        "        # for the last we dont have skip connection -> use simple decoder block\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, *features):\n",
        "\n",
        "        features = features[1:]  # remove first skip with same spatial resolution\n",
        "        features = features[::-1]  # reverse channels to start from head of encoder\n",
        "\n",
        "        head = features[0]\n",
        "        skips = features[1:]\n",
        "\n",
        "        x = self.center(head)\n",
        "\n",
        "        for i, decoder_block in enumerate(self.blocks):\n",
        "            skip = skips[i] if i < len(skips) else None\n",
        "            x = decoder_block(x, skip)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "719nnA3B5431"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Union, List\n",
        "\n",
        "from segmentation_models_pytorch.encoders import get_encoder\n",
        "from segmentation_models_pytorch.base import (\n",
        "    SegmentationModel,\n",
        "    SegmentationHead,\n",
        "    ClassificationHead,\n",
        ")\n",
        "\n",
        "class FPANet(SegmentationModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = \"resnet34\",\n",
        "        encoder_depth: int = 5,\n",
        "        encoder_weights: Optional[str] = \"imagenet\",\n",
        "        decoder_use_batchnorm: bool = True,\n",
        "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
        "        decoder_pab_channels: int = 64,\n",
        "        in_channels: int = 3,\n",
        "        classes: int = 1,\n",
        "        activation: Optional[Union[str, callable]] = None,\n",
        "        aux_params: Optional[dict] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = get_encoder(\n",
        "            encoder_name,\n",
        "            in_channels=in_channels,\n",
        "            depth=encoder_depth,\n",
        "            weights=encoder_weights,\n",
        "        )\n",
        "\n",
        "        self.decoder = FPAnetDecoder(\n",
        "            encoder_channels=self.encoder.out_channels,\n",
        "            decoder_channels=decoder_channels,\n",
        "            n_blocks=encoder_depth,\n",
        "            use_batchnorm=decoder_use_batchnorm,\n",
        "            pab_channels=decoder_pab_channels,\n",
        "        )\n",
        "\n",
        "        self.segmentation_head = SegmentationHead(\n",
        "            in_channels=decoder_channels[-1],\n",
        "            out_channels=classes,\n",
        "            activation=activation,\n",
        "            kernel_size=3,\n",
        "        )\n",
        "\n",
        "        if aux_params is not None:\n",
        "            self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n",
        "        else:\n",
        "            self.classification_head = None\n",
        "\n",
        "        self.name = \"manet-{}\".format(encoder_name)\n",
        "        self.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUQJX6j2ZVqX"
      },
      "source": [
        "Build Loss and Model (FPANet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeHijb_1ZaGb"
      },
      "outputs": [],
      "source": [
        "from segmentation_models_pytorch.losses import DiceLoss\n",
        "from segmentation_models_pytorch.metrics import get_stats, iou_score, accuracy, precision, recall, f1_score\n",
        "\n",
        "class SegmentationModelFPANet(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = FPANet(\n",
        "            encoder_name=\"resnet34\",\n",
        "            encoder_weights=\"imagenet\",\n",
        "            decoder_channels=(256, 128, 64, 32, 16),\n",
        "            in_channels=3,\n",
        "            classes=len(CLASSES),\n",
        "            activation=\"softmax\"\n",
        "        )\n",
        "        self.FreqFeature = FrequencyFeature()\n",
        "        self.criterion = DiceLoss(mode=\"multiclass\", from_logits=False)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        inputs = self.FreqFeature(inputs)\n",
        "        outputs = self.model(inputs)\n",
        "        if targets is not None:\n",
        "            loss = self.criterion(outputs, targets)\n",
        "            tp, fp, fn, tn = get_stats(outputs.argmax(dim=1).unsqueeze(1).type(torch.int64), targets, mode='multiclass', num_classes=len(CLASSES))\n",
        "            metrics = {\n",
        "                \"Accuracy\": accuracy(tp, fp, fn, tn, reduction=\"micro-imagewise\"),\n",
        "                \"IoU\": iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\"),\n",
        "                \"Precision\": precision(tp, fp, fn, tn, reduction=\"micro-imagewise\"),\n",
        "                \"Recall\": recall(tp, fp, fn, tn, reduction=\"micro-imagewise\"),\n",
        "                \"F1score\": f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
        "            }\n",
        "            return loss, metrics, outputs\n",
        "        else:\n",
        "            return outputs\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=0.0001)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, masks = batch\n",
        "\n",
        "        loss, metrics, outputs = self(images, masks)\n",
        "        self.log_dict({\n",
        "            \"train/Loss\": loss,\n",
        "            \"train/IoU\": metrics['IoU'],\n",
        "            \"train/Accuracy\": metrics['Accuracy'],\n",
        "            \"train/Precision\": metrics['Precision'],\n",
        "            \"train/Recall\": metrics['Recall'],\n",
        "            \"train/F1score\": metrics['F1score']\n",
        "        }, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, masks = batch\n",
        "\n",
        "        loss, metrics, outputs = self(images, masks)\n",
        "        self.log_dict({\n",
        "            \"val/Loss\": loss,\n",
        "            \"val/IoU\": metrics['IoU'],\n",
        "            \"val/Accuracy\": metrics['Accuracy'],\n",
        "            \"val/Precision\": metrics['Precision'],\n",
        "            \"val/Recall\": metrics['Recall'],\n",
        "            \"val/F1score\": metrics['F1score']\n",
        "        }, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, masks = batch\n",
        "\n",
        "        loss, metrics, outputs = self(images, masks)\n",
        "        self.log_dict({\n",
        "            \"test/Loss\": loss,\n",
        "            \"test/IoU\": metrics['IoU'],\n",
        "            \"test/Accuracy\": metrics['Accuracy'],\n",
        "            \"test/Precision\": metrics['Precision'],\n",
        "            \"test/Recall\": metrics['Recall'],\n",
        "            \"test/F1score\": metrics['F1score']\n",
        "        }, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avRgRnGYZdd3"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = SegmentationModelFPANet()\n",
        "summary(model, input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLEoZ1J1ZiiZ"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK1nXhnHZmgv"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "import math\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"drive/MyDrive/MLProject/LandCover/DeepGlobe/checkpoints\",\n",
        "    filename=\"best-checkpoint-FPANet\",\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"val/Loss\",\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "logger = CSVLogger(\"drive/MyDrive/MLProject/LandCover/DeepGlobe/lightning_logs\", name=\"landcover-log-FPANet\")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor=\"val/Loss\", patience=10)\n",
        "pl.seed_everything(42, )\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    log_every_n_steps=math.ceil(len(pd_train)/BATCH_SIZE),\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "    max_epochs=EPOCHS,\n",
        "    accelerator=\"auto\",\n",
        "    devices=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKhlpjgpFWwR"
      },
      "source": [
        "### Start Training (DeepGlobe Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATK7riDIZpLB"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjTUEUO2Zyr-"
      },
      "outputs": [],
      "source": [
        "metrics = pd.read_csv(\"drive/MyDrive/MLProject/LandCover/DeepGlobe/lightning_logs/landcover-log-FPANet/version_0/metrics.csv\")\n",
        "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "axes = [ax1, ax2, ax3, ax4, ax5, ax6]\n",
        "names = ['Loss', 'IoU', 'Accuracy', 'Precision', 'Recall', 'F1score']\n",
        "\n",
        "for axis, name in zip(axes, names):\n",
        "    epochs = list(range(len(metrics[f'train/{name}'].dropna())))\n",
        "    axis.plot(epochs, metrics[f'train/{name}'].dropna())\n",
        "    axis.plot(epochs, metrics[f'val/{name}'].dropna())\n",
        "    axis.set_title(f'{name}: Train/Val')\n",
        "    axis.set_ylabel(name)\n",
        "    axis.set_xlabel('Epoch')\n",
        "    #axis.set_xlim(0,5)\n",
        "    ax1.legend(['training', 'validation'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYmrYEL_FdjC"
      },
      "source": [
        "### Start Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Fco1GOZr-W"
      },
      "source": [
        "Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRz5_fDvZvRO"
      },
      "outputs": [],
      "source": [
        "trainer.test(model, ckpt_path=\"drive/MyDrive/MLProject/LandCover/DeepGlobe/checkpoints/best-checkpoint-FPANet.ckpt\", dataloaders = data_module.test_dataloader())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WXi642KEBEx"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-OntAQ6vQ_C"
      },
      "source": [
        "### Evaluate the model with single image from test dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "sh0n_mBCYV4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFC_zxiu-QZX"
      },
      "outputs": [],
      "source": [
        "model = SegmentationModelFPANet.load_from_checkpoint(\"drive/MyDrive/MLProject/LandCover/DeepGlobe/checkpoints/best-checkpoint-FPANet.ckpt\", map_location=torch.device(DEVICE))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xaPu5Dt-f-2"
      },
      "outputs": [],
      "source": [
        "image, mask = next(iter(data_module.test_dataloader()))\n",
        "image.shape, mask.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f67t-pxsm94q"
      },
      "outputs": [],
      "source": [
        "img1 = image[7,:,:,:]\n",
        "mask1 = mask[7,:,:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7kLUjPn3Zjx"
      },
      "outputs": [],
      "source": [
        "img1 = torch.unsqueeze(img1, axis=0)\n",
        "mask1 = torch.unsqueeze(mask1, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B2wWxim-nlA"
      },
      "outputs": [],
      "source": [
        "img1.shape, mask1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07XT-Xob-p9n"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    y_hat = model.to(DEVICE)(img1.to(DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w780xc99b9NM"
      },
      "outputs": [],
      "source": [
        "y_hat_1c = torch.argmax(y_hat, dim=1,keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xim3tV_LcTe7"
      },
      "outputs": [],
      "source": [
        "y_hat_1c.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jMSWpBIj5sN"
      },
      "outputs": [],
      "source": [
        "y_hat_1c_sqz = torch.squeeze(y_hat_1c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQrOI4fxkCpW"
      },
      "outputs": [],
      "source": [
        "y_hat_1c_sqz.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ5scWEulK1Q"
      },
      "outputs": [],
      "source": [
        "if DEVICE == \"cuda\":\n",
        "    y_hat_1c_sqz = y_hat_1c_sqz.to(\"cpu\")\n",
        "y_hat_final = category2rgb(y_hat_1c_sqz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYvcPDgjlT_P"
      },
      "outputs": [],
      "source": [
        "y_hat_final.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23I6WADLHETe"
      },
      "outputs": [],
      "source": [
        "mask1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGdnfDbvloei"
      },
      "outputs": [],
      "source": [
        "mask1_final = category2rgb(torch.squeeze(mask1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b902D6SKlzKd"
      },
      "outputs": [],
      "source": [
        "mask1_final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G-lSRn0lJFJ"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
        "\n",
        "ax1.set_title('Original mask')\n",
        "ax1.imshow(mask1_final)\n",
        "\n",
        "ax2.set_title('Prediction')\n",
        "ax2.imshow(y_hat_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0uwl7ySxEx3"
      },
      "source": [
        "### Evaluate the model with a random image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "PIDsb1fyYjYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6uhRDCcxYuU"
      },
      "outputs": [],
      "source": [
        "model = SegmentationModelFPANet.load_from_checkpoint(\"drive/MyDrive/MLProject/LandCover/DeepGlobe/checkpoints/best-checkpoint-FPANet.ckpt\", map_location=torch.device(DEVICE))\n",
        "model.eval()\n",
        "index = 190"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQAfR0_nxYuV"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_dim = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "img1 = cv2.imread(pd_test.iloc[index].IMAGES)\n",
        "img1 = cv2.resize(img1,img_dim)\n",
        "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "mask1 = cv2.imread(pd_test.iloc[index].MASKS)\n",
        "mask1 = cv2.resize(mask1,img_dim)\n",
        "mask1 = cv2.cvtColor(mask1, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIdes_bNxYuV"
      },
      "outputs": [],
      "source": [
        "img1.shape, mask1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7SNOwZt51Cd"
      },
      "outputs": [],
      "source": [
        "mask1 = rgb2category(mask1)\n",
        "\n",
        "img1 = np.transpose(img1, (2, 0, 1)).astype(np.float64)\n",
        "mask1 = np.expand_dims(mask1, axis=0)\n",
        "\n",
        "img1 = torch.Tensor(img1) / 255.0\n",
        "mask1 = torch.Tensor(mask1).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J5Aid686OQV"
      },
      "outputs": [],
      "source": [
        "img1.shape, mask1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibJvDbe36dQV"
      },
      "outputs": [],
      "source": [
        "img1 = torch.unsqueeze(img1, axis=0)\n",
        "mask1 = torch.unsqueeze(mask1, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNYup6_V6gaM"
      },
      "outputs": [],
      "source": [
        "img1.shape, mask1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D46E_6LKxYuV"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    y_hat = model.to(DEVICE)(img1.to(DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ctus7-6qqc"
      },
      "outputs": [],
      "source": [
        "y_hat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqD5nWDzxYuV"
      },
      "outputs": [],
      "source": [
        "y_hat_1c = torch.argmax(y_hat, dim=1,keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzOYlyFVxYuV"
      },
      "outputs": [],
      "source": [
        "y_hat_1c.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShtKVYttxYuV"
      },
      "outputs": [],
      "source": [
        "y_hat_1c_sqz = torch.squeeze(y_hat_1c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBlR137xYuV"
      },
      "outputs": [],
      "source": [
        "y_hat_1c_sqz.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p8OmJj-xYuV"
      },
      "outputs": [],
      "source": [
        "if DEVICE == \"cuda\":\n",
        "    y_hat_1c_sqz = y_hat_1c_sqz.to(\"cpu\")\n",
        "y_hat_final = category2rgb(y_hat_1c_sqz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqzA3K9jxYuV"
      },
      "outputs": [],
      "source": [
        "y_hat_final.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97AbG3p7xYuV"
      },
      "outputs": [],
      "source": [
        "mask1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNLctJ0KxYuW"
      },
      "outputs": [],
      "source": [
        "mask1_final = category2rgb(torch.squeeze(mask1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1PXU35ExYuW"
      },
      "outputs": [],
      "source": [
        "mask1_final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqPhJawrxYuW"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
        "\n",
        "ax1.set_title('Original mask')\n",
        "ax1.imshow(mask1_final)\n",
        "\n",
        "ax2.set_title('Prediction')\n",
        "ax2.imshow(y_hat_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Figures"
      ],
      "metadata": {
        "id": "bvZA7tTnuFF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SegmentationModelFPANet.load_from_checkpoint(\"drive/MyDrive/MLProject/LandCover/DeepGlobe/checkpoints/best-checkpoint-FPANet.ckpt\", map_location=torch.device(DEVICE))\n",
        "model.eval()\n",
        "test_dataset_len = len(pd_test)\n",
        "print(f'Test dataset length: {test_dataset_len}\\n')\n",
        "for index in range(test_dataset_len):\n",
        "  img_dim = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "  img1 = cv2.imread(pd_test.iloc[index].IMAGES)\n",
        "  img1 = cv2.resize(img1,img_dim)\n",
        "  img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "  #new\n",
        "  org_img = img1\n",
        "\n",
        "  mask1 = cv2.imread(pd_test.iloc[index].MASKS)\n",
        "  mask1 = cv2.resize(mask1,img_dim)\n",
        "  mask1 = cv2.cvtColor(mask1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  mask1 = rgb2category(mask1)\n",
        "\n",
        "  img1 = np.transpose(img1, (2, 0, 1)).astype(np.float64)\n",
        "  mask1 = np.expand_dims(mask1, axis=0)\n",
        "\n",
        "  img1 = torch.Tensor(img1) / 255.0\n",
        "  mask1 = torch.Tensor(mask1).long()\n",
        "\n",
        "  img1 = torch.unsqueeze(img1, axis=0)\n",
        "  mask1 = torch.unsqueeze(mask1, axis=0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_hat = model.to(DEVICE)(img1.to(DEVICE))\n",
        "\n",
        "  y_hat_1c = torch.argmax(y_hat, dim=1,keepdim=True)\n",
        "  y_hat_1c_sqz = torch.squeeze(y_hat_1c)\n",
        "\n",
        "  if DEVICE == \"cuda\":\n",
        "    y_hat_1c_sqz = y_hat_1c_sqz.to(\"cpu\")\n",
        "  y_hat_final = category2rgb(y_hat_1c_sqz)\n",
        "\n",
        "  mask1_final = category2rgb(torch.squeeze(mask1))\n",
        "  fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(16, 10))\n",
        "\n",
        "  ax0.set_title('image')\n",
        "  ax0.imshow(org_img)\n",
        "  ax0.axis('off')\n",
        "\n",
        "  ax1.set_title('Original mask')\n",
        "  ax1.imshow(mask1_final)\n",
        "  ax1.axis('off')\n",
        "\n",
        "  ax2.set_title('Prediction')\n",
        "  ax2.imshow(y_hat_final)\n",
        "  ax2.axis('off')\n",
        "  dir_path = \"drive/MyDrive/MLProject/LandCover/DeepGlobe/Output_Figures/FPANet\"\n",
        "  if not os.path.exists(dir_path):\n",
        "    os.makedirs(dir_path)\n",
        "  file_name = str(dir_path)+\"/FPANet_\" + str(index+1) + \".png\"\n",
        "  plt.savefig(file_name)\n",
        "  print(\"Saved Figure: \",index+1)\n",
        "  plt.close()\n"
      ],
      "metadata": {
        "id": "VtbT2fMvuDIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NapeXh-0qx4m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}